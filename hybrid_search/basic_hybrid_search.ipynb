{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Semantic AND Keyword Search (Hybrid Search)\n",
    "\n",
    "We will take a look at how to use Pinecone to perform a semantic search, while applying a traditional keyword search."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "all_sentences = [\n",
    "    \"purple is the best city in the forest\",\n",
    "    \"No way chimps go bananas for snacks!\",\n",
    "    \"it is not often you find soggy bananas on the street\",\n",
    "    \"green should have smelled more tranquil but somehow it just tasted rotten\",\n",
    "    \"joyce enjoyed eating pancakes with ketchup\",\n",
    "    \"throwing bananas on to the street is not art\",\n",
    "    \"as the asteroid hurtled toward earth becky was upset her dentist appointment had been canceled\",\n",
    "    \"I'm getting way too old. I don't even buy green bananas anymore.\",\n",
    "    \"to get your way you must not bombard the road with yellow fruit\",\n",
    "    \"Time flies like an arrow; fruit flies like a banana\"\n",
    "]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will use the `sentence-transformers` library to build our sentence embeddings. It can be installed using `pip` like so:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install sentence-transformers"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "*(The notebook may need to be restarted for the install to take effect)*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('flax-sentence-embeddings/all_datasets_v3_mpnet-base')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We use this pretrained sentence transformer model to encode the sentences."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "all_embeddings = model.encode(all_sentences)\n",
    "all_embeddings.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(10, 768)"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have **10** embeddings, each with a dimensionality of *768*. For the keyword search we will also need to store our sentences. But for the keyword search to be effective we should strip the sentences and break them into lists of words - eg, tokens."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from string import punctuation\n",
    "\n",
    "def tokenize(sentence):\n",
    "    return [w.lower().strip(punctuation) for w in sentence.split()]\n",
    "\n",
    "all_tokens = [tokenize(sentence) for sentence in all_sentences]\n",
    "all_tokens[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['purple', 'is', 'the', 'best', 'city', 'in', 'the', 'forest']"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have everything we need, the dense vector representations of each sentence, and the stripped list of tokens for each sentence. Let's prepare that data for *upserting* to Pinecone."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "data = []\n",
    "for i, (embedding, tokens) in enumerate(zip(all_embeddings, all_tokens)):\n",
    "    vector = {'id':f'{i}',\n",
    "              'values': embedding.tolist(),\n",
    "              'metadata':{'tokens':tokens}}\n",
    "    data.append(vector)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This produces a list of *10* dictionaries, each containing the embeddings and metadata for a single sample in the format:\n",
    "\n",
    "```json\n",
    "{\n",
    "    'id': 'sentence_n',\n",
    "    'values': [0.001, 0.002, ...],\n",
    "    'metadata': {\n",
    "        'tokens': ['purple', 'is', ...]\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "Next we need to connect to a Pinecone instance, you can get a [free API key here](https://app.pinecone.io)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import pinecone\n",
    "with open('../../../secret/pinecone', 'r') as fp:\n",
    "    api_key = fp.read()\n",
    "pinecone.init(api_key=api_key, environment='us-west1-gcp')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can check for existing indexes with:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "pinecone.list_indexes()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are none, so let's create a new index with `create_index` and connect with `Index`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "pinecone.create_index(name='keyword-search', dimension=all_embeddings.shape[1])\n",
    "index = pinecone.Index('keyword-search')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We reformat the list of dictionaries in `upserts` to a list of tuples, ready to be upserted."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "upserts = [(v['id'], v['values'], v['metadata']) for v in data]\n",
    "# then we upsert\n",
    "index.upsert(vectors=upserts)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'upsertedCount': 10.0}"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Querying\n",
    "\n",
    "We now have the data in our index, let's first perform a semantic search using a query sentence, we will return the most *semantically* similar sentences.\n",
    "\n",
    "We define the query, and encode as we did for `all_sentences` before."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "query_sentence = \"there is an art to getting your way and throwing bananas on to the street is not it\"\n",
    "xq = model.encode([query_sentence]).tolist()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "When querying with `index.query` we can pass a list of queries. We will pass the query vector as our first argument, and *later* when filtering for specific keywords we will add the `filter` parameter."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "result = index.query(xq, top_k=10, includeMetadata=True)\n",
    "result"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'results': [{'matches': [{'id': '5',\n",
       "                           'metadata': {'tokens': ['throwing',\n",
       "                                                   'bananas',\n",
       "                                                   'on',\n",
       "                                                   'to',\n",
       "                                                   'the',\n",
       "                                                   'street',\n",
       "                                                   'is',\n",
       "                                                   'not',\n",
       "                                                   'art']},\n",
       "                           'score': 0.732851863,\n",
       "                           'values': []},\n",
       "                          {'id': '8',\n",
       "                           'metadata': {'tokens': ['to',\n",
       "                                                   'get',\n",
       "                                                   'your',\n",
       "                                                   'way',\n",
       "                                                   'you',\n",
       "                                                   'must',\n",
       "                                                   'not',\n",
       "                                                   'bombard',\n",
       "                                                   'the',\n",
       "                                                   'road',\n",
       "                                                   'with',\n",
       "                                                   'yellow',\n",
       "                                                   'fruit']},\n",
       "                           'score': 0.574426889,\n",
       "                           'values': []},\n",
       "                      ...\n",
       "                          {'id': '6',\n",
       "                           'metadata': {'tokens': ['as',\n",
       "                                                   'the',\n",
       "                                                   'asteroid',\n",
       "                                                   'hurtled',\n",
       "                                                   'toward',\n",
       "                                                   'earth',\n",
       "                                                   'becky',\n",
       "                                                   'was',\n",
       "                                                   'upset',\n",
       "                                                   'her',\n",
       "                                                   'dentist',\n",
       "                                                   'appointment',\n",
       "                                                   'had',\n",
       "                                                   'been',\n",
       "                                                   'canceled']},\n",
       "                           'score': -0.0585537627,\n",
       "                           'values': []}],\n",
       "              'namespace': ''}]}"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's extract just the sentence IDs to see the order of what we have returned."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "[x['id'] for x in result['results'][0]['matches']]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['5', '8', '2', '1', '9', '7', '0', '3', '4', '6']"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's add a keyword filter. Let's restrict the search to only return sentences that contain the word `bananas`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "result = index.query(xq, top_k=10, filter={'tokens': 'bananas'})\n",
    "[x['id'] for x in result['results'][0]['matches']]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['5', '2', '1', '7']"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Again, let's extract IDs and then use these to see which sentences we're returning in the query above."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "ids = [int(x['id']) for x in result['results'][0]['matches']]\n",
    "for i in ids:\n",
    "    print(all_sentences[i])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "throwing bananas on to the street is not art\n",
      "it is not often you find soggy bananas on the street\n",
      "No way chimps go bananas for snacks!\n",
      "I'm getting way too old. I don't even buy green bananas anymore.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Okay cool, we can see that we're now filtering out all samples that do *not* contain the word 'bananas'. Maybe we'd like to extend this keyword filter further - for example we could filter for any samples that contain the word 'bananas' **OR** 'way' by modifying our filter to `{'$or': [{'tokens': 'bananas'}, {'tokens': 'way'}]}`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "result = index.query(xq, top_k=10, filter={'$or': [\n",
    "                         {'tokens': 'bananas'},\n",
    "                         {'tokens': 'way'}\n",
    "                     ]})\n",
    "\n",
    "ids = [int(x['id']) for x in result['results'][0]['matches']]\n",
    "for i in ids:\n",
    "    print(all_sentences[i])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "throwing bananas on to the street is not art\n",
      "to get your way you must not bombard the road with yellow fruit\n",
      "it is not often you find soggy bananas on the street\n",
      "No way chimps go bananas for snacks!\n",
      "I'm getting way too old. I don't even buy green bananas anymore.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Or we could decide we only want to return samples that contain *both* 'bananas' **AND** 'way' by swapping the `$or` modifier for `$and`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "result = index.query(xq, top_k=10, filter={'$and': [\n",
    "                         {'tokens': 'bananas'},\n",
    "                         {'tokens': 'way'}\n",
    "                     ]})\n",
    "\n",
    "ids = [int(x['id']) for x in result['results'][0]['matches']]\n",
    "for i in ids:\n",
    "    print(all_sentences[i])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "No way chimps go bananas for snacks!\n",
      "I'm getting way too old. I don't even buy green bananas anymore.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "If we have a lot of keywords including every single one manually like above can quickly get tiresome, so we can just write something like this instead:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "keywords = ['bananas', 'way', 'purple']\n",
    "filter_dict = [{'tokens': word} for word in keywords]\n",
    "filter_dict"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'tokens': 'bananas'}, {'tokens': 'way'}, {'tokens': 'purple'}]"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "And add it to our `query`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "result = index.query(xq, top_k=10, filter={'$or': filter_dict})\n",
    "\n",
    "ids = [int(x['id']) for x in result['results'][0]['matches']]\n",
    "for i in ids:\n",
    "    print(all_sentences[i])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "throwing bananas on to the street is not art\n",
      "to get your way you must not bombard the road with yellow fruit\n",
      "it is not often you find soggy bananas on the street\n",
      "No way chimps go bananas for snacks!\n",
      "I'm getting way too old. I don't even buy green bananas anymore.\n",
      "purple is the best city in the forest\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('ml': conda)"
  },
  "interpreter": {
   "hash": "a683edd788238e5c64f9fa2e4bdd4387776bc5c6f4f0a84da0685f9a25e421d6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}