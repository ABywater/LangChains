{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing Data\n",
    "\n",
    "Before we can perform question-answering (QA) operations we much initialize a document store which will store all of our source text and allow us to ask questions in reference to this source text. The process of initializing the document store and adding data to it is called *indexing*.\n",
    "\n",
    "## Load Data\n",
    "\n",
    "Before we begin indexing we must load and prepare our dataset. We are using an NHS dataset from Deepset, these same files can be downloaded from the demo repository. TK link\n",
    "\n",
    "We load them from the `/data` directory like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "paths = [str(x) for x in Path('../data').glob('*.txt')]\n",
    "paths[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are all plaintext files that contain NHS information on a particular condition like *pre-exlampsia*, *acute-lymphoblastic-leukaemia*, etc. Let's see how many of these files we have..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load them all into a variabled called `nhs_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nhs_text = []\n",
    "for path in paths:\n",
    "    with open(path, 'r') as fp:\n",
    "        text = fp.read()\n",
    "        nhs_text.append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what they look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nhs_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nhs_text[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly there is some data cleanup required to remove the generic text found on each page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nhs_text[0][:623]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nhs_text[2][:623]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nhs_text[5][:623]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "header = re.compile(r'(?s)Cookies.*\\nHome Health A to Z\\n')\n",
    "\n",
    "def clean(text):\n",
    "    text = header.sub('', text)\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "    text = text.split(\"Page last reviewed:\")[0]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean(nhs_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean(nhs_text[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cleaning function seems to work well enough so we can apply it to our data to returning a cleaner set of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nhs_text = [clean(text) for text in nhs_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our embedding model expects no more than 128 *tokens* of text, this translates to roughly 400-600 characters, so we will split the text into chunks of ~500 characters with a few extra conditions:\n",
    "\n",
    "* We split on newline characters `\\n`\n",
    "* We leave an overlap of one sentence between each chunk (to avoid missing relevant relationships between sentences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked = []\n",
    "chunk = 500\n",
    "\n",
    "for i, page in enumerate(nhs_text):\n",
    "    url = paths[i][8:-5].replace('_', '/')\n",
    "    page = page.split(\"\\n\")\n",
    "    context = \"\"\n",
    "    for j in range(len(page)):\n",
    "        if j != 0 and len(context) == 0:\n",
    "            context += page[j-1] + \" \"\n",
    "        context += page[j] + \" \"\n",
    "        if len(context) >= chunk:\n",
    "            chunked.append({\n",
    "                \"text\": context,\n",
    "                \"url\": url\n",
    "            })\n",
    "            context = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*If you'd like to keep things simpler, we can avoid adding the sentence overlap and cutting on newline characters with:*\n",
    "\n",
    "```python\n",
    "chunked = []\n",
    "chunk = 500\n",
    "\n",
    "for i, page in enumerate(nhs_text):\n",
    "    url = paths[i][8:]\n",
    "    for i in range(0, len(page), chunk):\n",
    "        i_end = min(i + chunk, len(page))\n",
    "        chunked.append({\n",
    "            \"text\": page[i:i_end],\n",
    "            \"url\": url\n",
    "        })\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the sentence overlap `\"If you're pregnant, hospitals and clinics are making sure it's safe for you to go to appointments.\"` shared between the two chunks. We also split chunks between sentences rather than mid-word (which would be the case if cutting on every 500 characters without consideration of newlines)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing the Data\n",
    "\n",
    "With our data prepared we can begin indexing it. First, we initialize the document store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.document_stores import PineconeDocumentStore\n",
    "\n",
    "API_KEY = \"<<YOUR_API_KEY>>\"\n",
    "INDEX_NAME = \"haystack-nhs-jul\"\n",
    "\n",
    "document_store = PineconeDocumentStore(\n",
    "    api_key=API_KEY,\n",
    "    index=INDEX_NAME,\n",
    "    similarity=\"dot_product\",\n",
    "    embedding_dim=768,\n",
    "    metadata_config={\"indexed\": [\"url\"]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we create our documents and add them to the document store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Document\n",
    "from tqdm.auto import tqdm  # progress bar\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "counter = 0\n",
    "docs = []\n",
    "\n",
    "for d in tqdm(chunked):\n",
    "    # create haystack document object with text content and doc metadata\n",
    "    doc = Document(\n",
    "        content=d[\"text\"],\n",
    "        meta={\n",
    "            \"url\": d[\"url\"]\n",
    "        }\n",
    "    )\n",
    "    docs.append(doc)\n",
    "    counter += 1\n",
    "    if counter % batch_size == 0:\n",
    "        # writing docs everytime 10k docs are reached\n",
    "        document_store.write_documents(docs)\n",
    "        docs.clear()\n",
    "\n",
    "if len(docs) > 0:\n",
    "    document_store.write_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we have added the documents to our document store, they do not include *embeddings*, which are required for us to perform a vector search. To create these embeddings we need to initialize a retriever model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.retriever.dense import EmbeddingRetriever\n",
    "\n",
    "retriever = EmbeddingRetriever(\n",
    "    document_store=document_store,\n",
    "    embedding_model='sentence-transformers/multi-qa-mpnet-base-dot-v1',\n",
    "    model_format=\"sentence_transformers\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use this retriever model to create the embeddings by calling the `update_embeddings` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store.update_embeddings(\n",
    "    retriever,\n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that our documents have been added using a simple `DocumentSearchPipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.pipelines import DocumentSearchPipeline\n",
    "from haystack.utils import print_documents\n",
    "\n",
    "search_pipe = DocumentSearchPipeline(retriever)\n",
    "result = search_pipe.run(\n",
    "    query=\"Who is affected by pre-eclampsia?\",\n",
    "    params={\"Retriever\": {\"top_k\": 2}}\n",
    ")\n",
    "\n",
    "print_documents(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This all looks good and we're returning relevant information for our query.\n",
    "\n",
    "This is just the indexing step, eg adding the data/documents + their embeddings to our document store. In the following notebook *01_test_pipeline.ipynb* we will initialize and test a full *extractive QA* pipeline.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b8e7999f96e1b425e2d542f21b571f5a4be3e97158b0b46ea1b2500df63956ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
