{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cc93d05f",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/generation/langchain/handbook/03-langchain-conversational-memory.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/generation/langchain/handbook/03-langchain-conversational-memory.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hcqKO0aI6_PI",
      "metadata": {
        "id": "hcqKO0aI6_PI"
      },
      "source": [
        "#### [LangChain Handbook](https://pinecone.io/learn/langchain)\n",
        "\n",
        "# Conversational Memory\n",
        "\n",
        "Conversational memory is how chatbots can respond to our queries in a chat-like manner. It enables a coherent conversation, and without it, every query would be treated as an entirely independent input without considering past interactions.\n",
        "\n",
        "The memory allows a _\"agent\"_ to remember previous interactions with the user. By default, agents are *stateless* — meaning each incoming query is processed independently of other interactions. The only thing that exists for a stateless agent is the current input, nothing else.\n",
        "\n",
        "There are many applications where remembering previous interactions is very important, such as chatbots. Conversational memory allows us to do that.\n",
        "\n",
        "In this notebook we'll explore this form of memory in the context of the LangChain library.\n",
        "\n",
        "We'll start by importing all of the libraries that we'll be using in this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "uZR3iGJJtdDE",
      "metadata": {
        "id": "uZR3iGJJtdDE"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain openai tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "66fb9c2a",
      "metadata": {
        "id": "66fb9c2a"
      },
      "outputs": [],
      "source": [
        "import inspect\n",
        "\n",
        "from getpass import getpass\n",
        "from langchain import OpenAI\n",
        "from langchain.chains import LLMChain, ConversationChain\n",
        "from langchain.chains.conversation.memory import (ConversationBufferMemory, \n",
        "                                                  ConversationSummaryMemory, \n",
        "                                                  ConversationBufferWindowMemory,\n",
        "                                                  ConversationKGMemory)\n",
        "from langchain.callbacks import get_openai_callback\n",
        "import tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wPdWz1IdxyBR",
      "metadata": {
        "id": "wPdWz1IdxyBR"
      },
      "source": [
        "To run this notebook, we will need to use an OpenAI LLM. Here we will setup the LLM we will use for the whole notebook, just input your openai api key when prompted. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c02c4fa2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c02c4fa2",
        "outputId": "ac8c8ddb-745a-4bb7-cf2e-cf7d1a8d9ddb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ],
      "source": [
        "OPENAI_API_KEY = getpass()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "baaa74b8",
      "metadata": {
        "id": "baaa74b8"
      },
      "outputs": [],
      "source": [
        "llm = OpenAI(\n",
        "    temperature=0, \n",
        "    openai_api_key=OPENAI_API_KEY,\n",
        "    model_name='gpt-3.5-turbo'  # can be used with llms like 'text-davinci-003'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "309g_2pqxzzB",
      "metadata": {
        "id": "309g_2pqxzzB"
      },
      "source": [
        "Later we will make use of a `count_tokens` utility function. This will allow us to count the number of tokens we are using for each call. We define it as so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "DsC3szr6yP3L",
      "metadata": {
        "id": "DsC3szr6yP3L"
      },
      "outputs": [],
      "source": [
        "def count_tokens(chain, query):\n",
        "    with get_openai_callback() as cb:\n",
        "        result = chain.run(query)\n",
        "        print(f'Spent a total of {cb.total_tokens} tokens')\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CnNF6i9r8RY_",
      "metadata": {
        "id": "CnNF6i9r8RY_"
      },
      "source": [
        "Now let's dive into **Conversational Memory**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e1f31b4",
      "metadata": {
        "id": "6e1f31b4"
      },
      "source": [
        "## What is memory?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b919c3a",
      "metadata": {
        "id": "5b919c3a"
      },
      "source": [
        "**Definition**: Memory is an agent's capacity of remembering previous interactions with the user (think chatbots)\n",
        "\n",
        "The official definition of memory is the following:\n",
        "\n",
        "\n",
        "> By default, Chains and Agents are stateless, meaning that they treat each incoming query independently. In some applications (chatbots being a GREAT example) it is highly important to remember previous interactions, both at a short term but also at a long term level. The concept of “Memory” exists to do exactly that.\n",
        "\n",
        "\n",
        "As we will see, although this sounds really straightforward there are several different ways to implement this memory capability."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3343a0e2",
      "metadata": {
        "id": "3343a0e2"
      },
      "source": [
        "Before we delve into the different memory modules that the library offers, we will introduce the chain we will be using for these examples: the `ConversationChain`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c9c13e9",
      "metadata": {
        "id": "6c9c13e9"
      },
      "source": [
        "As always, when understanding a chain it is interesting to peek into its prompt first and then take a look at its `._call` method. As we saw in the chapter on chains, we can check out the prompt by accessing the `template` within the `prompt` attribute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "96ff1ce3",
      "metadata": {
        "id": "96ff1ce3"
      },
      "outputs": [],
      "source": [
        "conversation = ConversationChain(\n",
        "    llm=llm, \n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "90ad394d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90ad394d",
        "outputId": "09dc89e4-d4b3-4771-fdfb-984c116e5b90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "{history}\n",
            "Human: {input}\n",
            "AI:\n"
          ]
        }
      ],
      "source": [
        "print(conversation.prompt.template)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f8b1e0c",
      "metadata": {
        "id": "9f8b1e0c"
      },
      "source": [
        "Interesting! So this chain's prompt is telling it to chat with the user and try to give truthful answers. If we look closely, there is a new component in the prompt that we didn't see when we were tinkering with the `LLMMathChain`: _history_. This is where our memory will come into play."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a7e7770",
      "metadata": {
        "id": "4a7e7770"
      },
      "source": [
        "What is this chain doing with this prompt? Let's take a look."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "43bfd2da",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43bfd2da",
        "outputId": "836c3efb-8b8f-4bcd-e4a9-791b02261f9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    def _call(self, inputs: Dict[str, Any]) -> Dict[str, str]:\n",
            "        known_values = self.prep_inputs(inputs.copy())\n",
            "        return self.apply([known_values])[0]\n",
            "     def apply(self, input_list: List[Dict[str, Any]]) -> List[Dict[str, str]]:\n",
            "        \"\"\"Utilize the LLM generate method for speed gains.\"\"\"\n",
            "        response = self.generate(input_list)\n",
            "        return self.create_outputs(response)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(inspect.getsource(conversation._call), inspect.getsource(conversation.apply))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84e664af",
      "metadata": {
        "id": "84e664af"
      },
      "source": [
        "Nothing really magical going on here, just a straightforward pass through an LLM. In fact, this chain inherits these methods directly from the `LLMChain` without any modification:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "d8f4aa79",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8f4aa79",
        "outputId": "32161f02-8635-4dc8-f9f8-bd7ac246b8a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    def _call(self, inputs: Dict[str, Any]) -> Dict[str, str]:\n",
            "        known_values = self.prep_inputs(inputs.copy())\n",
            "        return self.apply([known_values])[0]\n",
            "     def apply(self, input_list: List[Dict[str, Any]]) -> List[Dict[str, str]]:\n",
            "        \"\"\"Utilize the LLM generate method for speed gains.\"\"\"\n",
            "        response = self.generate(input_list)\n",
            "        return self.create_outputs(response)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(inspect.getsource(LLMChain._call), inspect.getsource(LLMChain.apply))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6aaa70bf",
      "metadata": {
        "id": "6aaa70bf"
      },
      "source": [
        "So basically this chain combines an input from the user with the conversation history to generate a meaningful (and hopefully truthful) response."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19f5172f",
      "metadata": {
        "id": "19f5172f"
      },
      "source": [
        "Now that we've understood the basics of the chain we'll be using, we can get into memory. Let's dive in!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f1a33f6",
      "metadata": {
        "id": "0f1a33f6"
      },
      "source": [
        "## Memory types"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d732b7a",
      "metadata": {
        "id": "4d732b7a"
      },
      "source": [
        "In this section we will review several memory types and analyze the pros and cons of each one, so you can choose the best one for your use case."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04d70642",
      "metadata": {
        "id": "04d70642"
      },
      "source": [
        "### Memory type #1: ConversationBufferMemory"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53d3cb2b",
      "metadata": {
        "id": "53d3cb2b"
      },
      "source": [
        "The `ConversationBufferMemory` does just what its name suggests: it keeps a buffer of the previous conversation excerpts as part of the context in the prompt."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d80a974a",
      "metadata": {
        "id": "d80a974a"
      },
      "source": [
        "**Key feature:** _the conversation buffer memory keeps the previous pieces of conversation completely unmodified, in their raw form._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "2267f1f0",
      "metadata": {
        "id": "2267f1f0"
      },
      "outputs": [],
      "source": [
        "conversation_buf = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=ConversationBufferMemory()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lseziAMcAyvX",
      "metadata": {
        "id": "lseziAMcAyvX"
      },
      "source": [
        "We pass a user prompt the the `ConversationBufferMemory` like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "M0cwooC5A5Id",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0cwooC5A5Id",
        "outputId": "5f82b26e-969a-4b07-e888-32df1f880493"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input': 'Good morning AI!',\n",
              " 'history': '',\n",
              " 'response': 'Good morning! How can I assist you today?'}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation_buf(\"Good morning AI!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xlKINTFYA9eo",
      "metadata": {
        "id": "xlKINTFYA9eo"
      },
      "source": [
        "This one call used a total of `85` tokens, but we can't see that from the above. If we'd like to count the number of tokens being used we just pass our conversation chain object and the message we'd like to input via the `count_tokens` function we defined earlier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "d1bd5a88",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "d1bd5a88",
        "outputId": "cb22c24e-d8c7-45f4-c201-b9c593af6044"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spent a total of 169 tokens\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"That's a fascinating topic! Large Language Models, such as GPT-3, have shown great promise in generating human-like text. However, integrating external knowledge can enhance their capabilities even further. There are several approaches to achieving this, such as using knowledge graphs or incorporating structured data. Would you like me to provide more details on these approaches?\""
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_buf, \n",
        "    \"My interest here is to explore the potential of integrating Large Language Models with external knowledge\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "146170ca",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "146170ca",
        "outputId": "4eb76f1f-2544-413c-d951-ad7599124b85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spent a total of 277 tokens\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Sure! One approach is to use pre-existing knowledge graphs, such as Wikidata or DBpedia, to provide additional context to the language model. Another approach is to incorporate structured data, such as tables or databases, into the model's training data. Additionally, some researchers are exploring the use of reinforcement learning to teach the model how to interact with external knowledge sources. These are just a few examples, but there are many other possibilities to explore.\""
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_buf,\n",
        "    \"I just want to analyze the different possibilities. What can you think of?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "3e15411a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "3e15411a",
        "outputId": "c0328913-6bcd-46cc-d7a2-01dc1e7c066b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spent a total of 399 tokens\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'There are many different types of data sources that could be used to give context to the model. Some examples include structured data sources like databases or tables, unstructured data sources like text documents or web pages, and knowledge graphs like Wikidata or DBpedia. Additionally, some researchers are exploring the use of social media data or user-generated content to provide additional context to the model. Ultimately, the choice of data source will depend on the specific use case and the type of information that is most relevant to the task at hand.'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_buf, \n",
        "    \"Which data source types could be used to give context to the model?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "3352cc48",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "3352cc48",
        "outputId": "5275bd2e-fe34-4974-8e09-7947b7ad58d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spent a total of 446 tokens\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Your aim is to explore the potential of integrating Large Language Models with external knowledge. You want to analyze the different possibilities and data sources that could be used to give context to the model.'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_buf, \n",
        "    \"What is my aim again?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "431b74ff",
      "metadata": {
        "id": "431b74ff"
      },
      "source": [
        "Our LLM with `ConversationBufferMemory` can clearly remember earlier interactions in the conversation. Let's take a closer look to how the LLM is saving our previous conversation. We can do this by accessing the `.buffer` attribute for the `.memory` in our chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "984afd09",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "984afd09",
        "outputId": "15f9b1c9-dc37-47a8-8a40-40afa4e0ce25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Human: Good morning AI!\n",
            "AI: Good morning! How can I assist you today?\n",
            "Human: My interest here is to explore the potential of integrating Large Language Models with external knowledge\n",
            "AI: That's a fascinating topic! Large Language Models, such as GPT-3, have shown great promise in generating human-like text. However, integrating external knowledge can enhance their capabilities even further. There are several approaches to achieving this, such as using knowledge graphs or incorporating structured data. Would you like me to provide more details on these approaches?\n",
            "Human: I just want to analyze the different possibilities. What can you think of?\n",
            "AI: Sure! One approach is to use pre-existing knowledge graphs, such as Wikidata or DBpedia, to provide additional context to the language model. Another approach is to incorporate structured data, such as tables or databases, into the model's training data. Additionally, some researchers are exploring the use of reinforcement learning to teach the model how to interact with external knowledge sources. These are just a few examples, but there are many other possibilities to explore.\n",
            "Human: Which data source types could be used to give context to the model?\n",
            "AI: There are many different types of data sources that could be used to give context to the model. Some examples include structured data sources like databases or tables, unstructured data sources like text documents or web pages, and knowledge graphs like Wikidata or DBpedia. Additionally, some researchers are exploring the use of social media data or user-generated content to provide additional context to the model. Ultimately, the choice of data source will depend on the specific use case and the type of information that is most relevant to the task at hand.\n",
            "Human: What is my aim again?\n",
            "AI: Your aim is to explore the potential of integrating Large Language Models with external knowledge. You want to analyze the different possibilities and data sources that could be used to give context to the model.\n"
          ]
        }
      ],
      "source": [
        "print(conversation_buf.memory.buffer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4570267d",
      "metadata": {
        "id": "4570267d"
      },
      "source": [
        "Nice! So every piece of our conversation has been explicitly recorded and sent to the LLM in the prompt."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acf1a90b",
      "metadata": {
        "id": "acf1a90b"
      },
      "source": [
        "### Memory type #2: ConversationSummaryMemory"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01f61fe9",
      "metadata": {
        "id": "01f61fe9"
      },
      "source": [
        "The problem with the `ConversationBufferMemory` is that as the conversation progresses, the token count of our context history adds up. This is problematic because we might max out our LLM with a prompt that is too large to be processed."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0516c7d4",
      "metadata": {
        "id": "0516c7d4"
      },
      "source": [
        "Enter `ConversationSummaryMemory`.\n",
        "\n",
        "Again, we can infer from the name what is going on.. we will keep a summary of our previous conversation snippets as our history. How will we summarize these? LLM to the rescue."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86b0a905",
      "metadata": {
        "id": "86b0a905"
      },
      "source": [
        "**Key feature:** _the conversation summary memory keeps the previous pieces of conversation in a summarized form, where the summarization is performed by an LLM._"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ea6050c",
      "metadata": {
        "id": "0ea6050c"
      },
      "source": [
        "In this case we need to send the llm to our memory constructor to power its summarization ability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "f33a16a7",
      "metadata": {
        "id": "f33a16a7"
      },
      "outputs": [],
      "source": [
        "conversation_sum = ConversationChain(\n",
        "    llm=llm, \n",
        "    memory=ConversationSummaryMemory(llm=llm)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b64c4896",
      "metadata": {
        "id": "b64c4896"
      },
      "source": [
        "When we have an llm, we always have a prompt ;) Let's see what's going on inside our conversation summary memory:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "c476824d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c476824d",
        "outputId": "adc62f41-aaca-4988-8eb8-baedbb8faeef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\n",
            "\n",
            "EXAMPLE\n",
            "Current summary:\n",
            "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n",
            "\n",
            "New lines of conversation:\n",
            "Human: Why do you think artificial intelligence is a force for good?\n",
            "AI: Because artificial intelligence will help humans reach their full potential.\n",
            "\n",
            "New summary:\n",
            "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\n",
            "END OF EXAMPLE\n",
            "\n",
            "Current summary:\n",
            "{summary}\n",
            "\n",
            "New lines of conversation:\n",
            "{new_lines}\n",
            "\n",
            "New summary:\n"
          ]
        }
      ],
      "source": [
        "print(conversation_sum.memory.prompt.template)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df90cdf3",
      "metadata": {
        "id": "df90cdf3"
      },
      "source": [
        "Cool! So each new interaction is summarized and appended to a running summary as the memory of our chain. Let's see how this works in practice!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "34343665",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "34343665",
        "outputId": "358da670-ae13-4548-9d85-6c8674877f9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spent a total of 252 tokens\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Good morning! How can I assist you today?'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# without count_tokens we'd call `conversation_sum(\"Good morning AI!\")`\n",
        "# but let's keep track of our tokens:\n",
        "count_tokens(\n",
        "    conversation_sum, \n",
        "    \"Good morning AI!\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "b757bba3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "b757bba3",
        "outputId": "04351bdc-3430-44de-9cf0-610979904022"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spent a total of 602 tokens\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"That's a fascinating topic! Large Language Models, such as GPT-3, have shown great promise in generating human-like text. When it comes to integrating external knowledge, there are a few approaches that can be taken. One is to use pre-existing knowledge graphs, such as Wikidata or DBpedia, to augment the model's understanding of the world. Another approach is to fine-tune the model on a specific domain, such as medicine or law, to give it a deeper understanding of the relevant concepts and terminology. Would you like me to provide more information on these approaches, or do you have any specific questions in mind?\""
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_sum, \n",
        "    \"My interest here is to explore the potential of integrating Large Language Models with external knowledge\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "d0a373e2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "d0a373e2",
        "outputId": "df48d632-091c-4c7c-dc71-3e0079111c83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spent a total of 714 tokens\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Well, there are a few different approaches you could take when it comes to integrating external knowledge with Large Language Models. One option is to use pre-existing knowledge graphs, which can provide a structured representation of knowledge that can be easily integrated with the model. Another approach is to fine-tune the model on a specific domain, which can help it better understand the specific concepts and terminology related to that domain. Of course, there are pros and cons to each approach, and the best option will depend on your specific use case. If you have any specific questions or would like more information on either approach, I'd be happy to provide it.\""
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_sum, \n",
        "    \"I just want to analyze the different possibilities. What can you think of?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "2e286f0d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "2e286f0d",
        "outputId": "e04225cb-9550-411e-b4c9-87bb9e54eb2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spent a total of 741 tokens\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'There are many different types of data sources that could be used to give context to a Large Language Model. Some examples include structured data sources like knowledge graphs or databases, unstructured data sources like text corpora or social media feeds, and even sensor data or other types of real-time data streams. The choice of data source will depend on the specific use case and the type of information that is most relevant to the task at hand. Would you like me to provide more information on any of these options?'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_sum, \n",
        "    \"Which data source types could be used to give context to the model?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "891180f2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "891180f2",
        "outputId": "81b460c6-50a4-45eb-d7f0-d663285837cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spent a total of 689 tokens\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"I'm sorry, I don't have that information. Could you please clarify your question or provide more context?\""
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_sum, \n",
        "    \"What is my aim again?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "2d768e44",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d768e44",
        "outputId": "e9340c6c-8675-4094-d171-af0efd4eaaf3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The human expresses interest in integrating Large Language Models with external knowledge, and the AI suggests two approaches: using pre-existing knowledge graphs or fine-tuning the model on a specific domain. The AI offers to provide more information or answer any specific questions the human may have. The human asks for more possibilities, and the AI explains that there are many different types of data sources that could be used to give context to a Large Language Model. The AI emphasizes that the choice of data source will depend on the specific use case and the type of information that is most relevant to the task at hand. The human then asks for clarification on their aim, to which the AI apologizes for not having that information and asks for more context.\n"
          ]
        }
      ],
      "source": [
        "print(conversation_sum.memory.buffer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0dd35c8c",
      "metadata": {
        "id": "0dd35c8c"
      },
      "source": [
        "You might be wondering.. if the aggregate token count is greater in each call here than in the buffer example, why should we use this type of memory? Well, if we check out buffer we will realize that although we are using more tokens in each instance of our conversation, our final history is shorter. This will enable us to have many more interactions before we reach our prompt's max length, making our chatbot more robust to longer conversations.\n",
        "\n",
        "We can count the number of tokens being used (without making a call to OpenAI) using the `tiktoken` tokenizer like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "nzijj4RZFX3I",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzijj4RZFX3I",
        "outputId": "899cbaa2-ecce-41b2-ba5c-729632adfe60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Buffer memory conversation length: 386\n",
            "Summary memory conversation length: 144\n"
          ]
        }
      ],
      "source": [
        "# initialize tokenizer\n",
        "tokenizer = tiktoken.encoding_for_model('gpt-3.5-turbo')\n",
        "\n",
        "# show number of tokens for the memory used by each memory type\n",
        "print(\n",
        "    f'Buffer memory conversation length: {len(tokenizer.encode(conversation_buf.memory.buffer))}\\n'\n",
        "    f'Summary memory conversation length: {len(tokenizer.encode(conversation_sum.memory.buffer))}'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bab0c09",
      "metadata": {
        "id": "2bab0c09"
      },
      "source": [
        "_Practical Note: the `text-davinci-003` model [has](https://platform.openai.com/docs/api-reference/completions/create#completions/create-max_tokens) a large max tokens count of 4096 tokens between prompt and answer._"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "494830ea",
      "metadata": {
        "id": "494830ea"
      },
      "source": [
        "### Memory type #3: ConversationBufferWindowMemory"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00762844",
      "metadata": {
        "id": "00762844"
      },
      "source": [
        "Another great option for these cases is the `ConversationBufferWindowMemory` where we will be keeping a few of the last interactions in our memory but we will intentionally drop the oldest ones - short-term memory if you'd like. Here the aggregate token count **and** the per-call token count will drop noticeably. We will control this window with the `k` parameter."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "206a5915",
      "metadata": {
        "id": "206a5915"
      },
      "source": [
        "**Key feature:** _the conversation buffer window memory keeps the latest pieces of the conversation in raw form_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "45be373a",
      "metadata": {
        "id": "45be373a"
      },
      "outputs": [],
      "source": [
        "conversation_bufw = ConversationChain(\n",
        "    llm=llm, \n",
        "    memory=ConversationBufferWindowMemory(k=2)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "fc4dd8a0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "fc4dd8a0",
        "outputId": "70ebe205-a410-4a19-f50a-86193360eb69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spent a total of 79 tokens\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Good morning! How can I assist you today?'"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_bufw, \n",
        "    \"Good morning AI!\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "b9992e8d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "b9992e8d",
        "outputId": "1c677025-ba5e-49c6-9f62-8306a4ffd3fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spent a total of 169 tokens\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"That's a fascinating topic! Large Language Models, such as GPT-3, have shown great promise in generating human-like text. However, integrating external knowledge can enhance their capabilities even further. There are several approaches to achieving this, such as using knowledge graphs or incorporating structured data. Would you like me to provide more details on these approaches?\""
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_bufw, \n",
        "    \"My interest here is to explore the potential of integrating Large Language Models with external knowledge\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "3f2e98d9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "3f2e98d9",
        "outputId": "b2fdc1db-32f8-410b-ac5f-77ede3c57bd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spent a total of 277 tokens\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Sure! One approach is to use pre-existing knowledge graphs, such as DBpedia or Wikidata, to provide additional context to the language model. Another approach is to incorporate structured data, such as tables or databases, into the model's training data. Additionally, some researchers are exploring the use of reinforcement learning to teach the model how to interact with external knowledge sources. These are just a few examples, but there are many other possibilities to explore.\""
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_bufw, \n",
        "    \"I just want to analyze the different possibilities. What can you think of?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "a2a8d062",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "a2a8d062",
        "outputId": "c677e6dc-ee82-4b0c-e6c8-61589feec603"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spent a total of 343 tokens\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"There are many different types of data sources that could be used to give context to the model. Some examples include knowledge graphs, databases, tables, text corpora, and even social media data. The choice of data source will depend on the specific use case and the type of information that is needed to enhance the model's capabilities.\""
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_bufw, \n",
        "    \"Which data source types could be used to give context to the model?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "ff199a3f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "ff199a3f",
        "outputId": "dc65be25-2d4f-4265-8fe4-13dd118de195"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spent a total of 292 tokens\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"I'm sorry, I don't have enough context to know what your specific aim is. Could you please provide more information or clarify your question?\""
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_bufw, \n",
        "    \"What is my aim again?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5f59f77",
      "metadata": {
        "id": "f5f59f77"
      },
      "source": [
        "As we can see, it effectively 'fogot' what we talked about in the first interaction. Let's see what it 'remembers'. Given that we set k to be `2`, we would expect it remembers only our last two interactions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b354c8d",
      "metadata": {
        "id": "6b354c8d"
      },
      "source": [
        "We need to access a special method here since, in this memory type, the buffer is first passed through this method to be sent later to the llm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "85266406",
      "metadata": {
        "id": "85266406"
      },
      "outputs": [],
      "source": [
        "bufw_history = conversation_bufw.memory.load_memory_variables(\n",
        "    inputs=[]\n",
        ")['history']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "5904ae2a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5904ae2a",
        "outputId": "54c917ab-82df-42d4-87f0-c5637ef48bb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Human: Which data source types could be used to give context to the model?\n",
            "AI: There are many different types of data sources that could be used to give context to the model. Some examples include knowledge graphs, databases, tables, text corpora, and even social media data. The choice of data source will depend on the specific use case and the type of information that is needed to enhance the model's capabilities.\n",
            "Human: What is my aim again?\n",
            "AI: I'm sorry, I don't have enough context to know what your specific aim is. Could you please provide more information or clarify your question?\n"
          ]
        }
      ],
      "source": [
        "print(bufw_history)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae8b937d",
      "metadata": {
        "id": "ae8b937d"
      },
      "source": [
        "Makes sense. \n",
        "\n",
        "On the plus side, we are shortening our conversation length when compared to buffer memory _without_ a window:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "9fbb50fe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fbb50fe",
        "outputId": "e0fe56da-47a6-45e1-9bd1-85aa50850b51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Buffer memory conversation length: 386\n",
            "Summary memory conversation length: 144\n",
            "Buffer window memory conversation length: 123\n"
          ]
        }
      ],
      "source": [
        "print(\n",
        "    f'Buffer memory conversation length: {len(tokenizer.encode(conversation_buf.memory.buffer))}\\n'\n",
        "    f'Summary memory conversation length: {len(tokenizer.encode(conversation_sum.memory.buffer))}\\n'\n",
        "    f'Buffer window memory conversation length: {len(tokenizer.encode(bufw_history))}'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69842cc1",
      "metadata": {
        "id": "69842cc1"
      },
      "source": [
        "_Practical Note: We are using `k=2` here for illustrative purposes, in most real world applications you would need a higher value for k._"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2aea5fc8",
      "metadata": {
        "id": "2aea5fc8"
      },
      "source": [
        "### More memory types!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "daeb5162",
      "metadata": {
        "id": "daeb5162"
      },
      "source": [
        "Given that we understand memory already, we will present a few more memory types here and hopefully a brief description will be enough to understand their underlying functionality."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0365333",
      "metadata": {
        "id": "f0365333"
      },
      "source": [
        "#### ConversationSummaryBufferMemory"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "317f298e",
      "metadata": {
        "id": "317f298e"
      },
      "source": [
        "**Key feature:** _the conversation summary memory keeps a summary of the earliest pieces of conversation while retaining a raw recollection of the latest interactions._"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57ef5c8b",
      "metadata": {
        "id": "57ef5c8b"
      },
      "source": [
        "#### ConversationKnowledgeGraphMemory"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40248f03",
      "metadata": {
        "id": "40248f03"
      },
      "source": [
        "This is a super cool memory type that was introduced just [recently](https://twitter.com/LangChainAI/status/1625158388824043522). It is based on the concept of a _knowledge graph_ which recognizes different entities and connects them in pairs with a predicate resulting in (subject, predicate, object) triplets. This enables us to compress a lot of information into highly significant snippets that can be fed into the model as context. If you want to understand this memory type in more depth you can check out [this](https://apex974.com/articles/explore-langchain-support-for-knowledge-graph) blogpost."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91952cd1",
      "metadata": {
        "id": "91952cd1"
      },
      "source": [
        "**Key feature:** _the conversation knowledge graph memory keeps a knowledge graph of all the entities that have been mentioned in the interactions together with their semantic relationships._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "02241bc3",
      "metadata": {
        "id": "02241bc3"
      },
      "outputs": [],
      "source": [
        "# you may need to install this library\n",
        "# !pip install -qU networkx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "c5f10a89",
      "metadata": {
        "id": "c5f10a89"
      },
      "outputs": [],
      "source": [
        "conversation_kg = ConversationChain(\n",
        "    llm=llm, \n",
        "    memory=ConversationKGMemory(llm=llm)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "65957fe2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "65957fe2",
        "outputId": "9fe039f8-179f-499e-a5c6-829ad5f30773"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spent a total of 1480 tokens\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Hello, Human! It's nice to meet you. Mangoes are a delicious fruit. Did you know that they are native to South Asia and have been cultivated for over 4,000 years? They are also a great source of vitamin C and fiber. Do you have a favorite way to eat mangoes?\""
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_kg, \n",
        "    \"My name is human and I like mangoes!\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74054534",
      "metadata": {
        "id": "74054534"
      },
      "source": [
        "The memory keeps a knowledge graph of everything it learned so far."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "5a8c54fb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a8c54fb",
        "outputId": "20eea5c0-de1e-441e-ab80-e0d88055988c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Human', 'human', 'has a name'), ('Human', 'mangoes', 'likes')]"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation_kg.memory.kg.get_triples()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1a1ca15",
      "metadata": {
        "id": "e1a1ca15"
      },
      "source": [
        "#### ConversationEntityMemory"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41e9aeaf",
      "metadata": {
        "id": "41e9aeaf"
      },
      "source": [
        "**Key feature:** _the conversation entity memory keeps a recollection of the main entities that have been mentioned, together with their specific attributes._"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2900a385",
      "metadata": {
        "id": "2900a385"
      },
      "source": [
        "The way this works is quite similar to the `ConversationKnowledgeGraphMemory`, you can refer to the [docs](https://langchain.readthedocs.io/en/latest/modules/memory/examples/entity_summary_memory.html) if you want to see it in action. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d45112bd",
      "metadata": {
        "id": "d45112bd"
      },
      "source": [
        "## What else can we do with memory?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78296bff",
      "metadata": {
        "id": "78296bff"
      },
      "source": [
        "There are several cool things we can do with memory in langchain. We can:\n",
        "* implement our own custom memory module\n",
        "* use multiple memory modules in the same chain\n",
        "* combine agents with memory and other tools\n",
        "\n",
        "If this piques your interest, we suggest you to go take a look at the memory [how-to](https://langchain.readthedocs.io/en/latest/modules/memory/how_to_guides.html) section in the docs!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
