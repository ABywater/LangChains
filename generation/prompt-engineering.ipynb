{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain openai cohere"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering\n",
    "\n",
    "In this notebook we'll explore the fundamentals of prompt engineering.\n",
    "\n",
    "## Structure of a Prompt\n",
    "\n",
    "A prompt can consist of multiple components:\n",
    "\n",
    "* Instructions\n",
    "* External information or context\n",
    "* User input or query\n",
    "* Output indicator\n",
    "\n",
    "Not all prompts require all of these components, but often a good prompt will use two or more of them. Let's define what they all are more precisely.\n",
    "\n",
    "**Instructions** tell the model what to do, typically how it should use inputs and/or external information to produce the output we want.\n",
    "\n",
    "**External information or context** are additional information that we either manually insert into the prompt, retrieve via a vector database (long-term memory), or pull in through other means (API calls, calculations, etc).\n",
    "\n",
    "**User input or query** is typically a query directly input by the user of the system.\n",
    "\n",
    "**Output indicator** is the *beginning* of the generated text. For a model generating Python code we may put `import ` (as most Python scripts begin with a library `import`), or a chatbot may begin with `Chatbot: ` (assuming we format the chatbot script as lines of interchanging text between `User` and `Chatbot`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# core of a prompt usually contains:\n",
    "# - task description\n",
    "# - current input\n",
    "# - output *indicator*\n",
    "template = \"\"\"\n",
    "Extract key programming tools and ML models from the context below:\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = (\n",
    "    \"To build a Q&A tool over YouTube videos we need to start by \" +\n",
    "    \"downloading videos and extracting their audio. To do this \" +\n",
    "    \"we use ffmpeg which will leave us with a set of audio files. \" +\n",
    "    \"From here we must transcibe the audio into text. To do this \" +\n",
    "    \"we used OpenAI's Whisper. We can use Whisper via Hugging \" +\n",
    "    \"Face's transformers library. After extracting the text we \" +\n",
    "    \"need to chunk them together into larger chunks of text. \" +\n",
    "    \"If using the sentence-transformers library to create \" +\n",
    "    \"vector embeddings (which we will use for semantic search) \" +\n",
    "    \"we need to chunk the text into sentence to paragraph sized \" +\n",
    "    \"chunks. Alternatively, if using LLMs like those from OpenAI \" +\n",
    "    \"we can use much larger chunks of text. Around 30 sentences \" +\n",
    "    \"is a good starting point. After chunking the text we can \" +\n",
    "    \"create vector embeddings for each chunk. As mentioned, we \" +\n",
    "    \"can use sentence-transformers or LLMs via OpenAI or Cohere. \" +\n",
    "    \"Once we have vector embeddings for each chunk we can use \" +\n",
    "    \"the Pinecone vector database to store the embeddings and \" +\n",
    "    \"search through them. Next we build a web interface using \" +\n",
    "    \"Streamlit or Gradio. Via the interface users should be able \" +\n",
    "    \"to pass in a question like 'How to train a sentence \" +\n",
    "    \"transformer?', this query will be encoded by the same \" +\n",
    "    \"embedding model used before, this embedding is passed to \" +\n",
    "    \"Pinecone to return the most similar chunks of text. These \" +\n",
    "    \"chunks of text are then passed to a generative model that\" +\n",
    "    \"will generate a natural language answer to the question. \" +\n",
    "    \"based on the returned information.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extract key programming tools and ML models from the context below:\n",
      "\n",
      "Context: To build a Q&A tool over YouTube videos we need to start by downloading videos and extracting their audio. To do this we use ffmpeg which will leave us with a set of audio files. From here we must transcibe the audio into text. To do this we used OpenAI's Whisper. We can use Whisper via Hugging Face's transformers library. After extracting the text we need to chunk them together into larger chunks of text. If using the sentence-transformers library to create vector embeddings (which we will use for semantic search) we need to chunk the text into sentence to paragraph sized chunks. Alternatively, if using LLMs like those from OpenAI we can use much larger chunks of text. Around 30 sentences is a good starting point. After chunking the text we can create vector embeddings for each chunk. As mentioned, we can use sentence-transformers or LLMs via OpenAI or Cohere. Once we have vector embeddings for each chunk we can use the Pinecone vector database to store the embeddings and search through them. Next we build a web interface using Streamlit or Gradio. Via the interface users should be able to pass in a question like 'How to train a sentence transformer?', this query will be encoded by the same embedding model used before, this embedding is passed to Pinecone to return the most similar chunks of text. These chunks of text are then passed to a generative model thatwill generate a natural language answer to the question. based on the returned information.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"context\"]\n",
    ")\n",
    "\n",
    "print(\n",
    "    prompt.format(context=context)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that we've created our prompt, let's see how it performs with OpenAI and Cohere generative models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "# initialize the models\n",
    "openai = OpenAI(\n",
    "    model_name=\"text-davinci-003\",\n",
    "    openai_api_key=\"API_KEY\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Programming tools: ffmpeg, Hugging Face's Transformers Library, Sentence-transformers Library, OpenAI, Cohere, Streamlit, Gradio \n",
      "\n",
      "ML models: OpenAI LLMs, Pinecone Vector Database, Generative Model\n"
     ]
    }
   ],
   "source": [
    "print(openai(prompt.format(context=context)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a good answer but the format of extracted information is not ideal, we can specify the template we'd like within the prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tools:\n",
      "  - ffmpeg\n",
      "  - OpenAI's Whisper\n",
      "  - Hugging Face's transformers library\n",
      "  - sentence-transformers library\n",
      "  - OpenAI's LLMs\n",
      "  - Cohere\n",
      "  - Pinecone vector database\n",
      "  - Streamlit\n",
      "  - Gradio\n",
      "Models:\n",
      "  - Sentence Transformer\n",
      "  - LLMs\n",
      "  - Generative Model\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "Extract key programming tools and ML models from the context below\n",
    "and display in the format:\n",
    "\n",
    "Keywords\n",
    "Tools:\n",
    "  - tool 1\n",
    "  - tool 2\n",
    "  - ...\n",
    "Models:\n",
    "  - model 1\n",
    "  - model 2\n",
    "  ...\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Keywords\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"context\"]\n",
    ")\n",
    "\n",
    "print(openai(prompt.format(context=context)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much better, we could also add some examples as \"few-shot\" training for the model like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tools:\n",
      "  - ffmpeg\n",
      "  - hugging face transformers library\n",
      "  - sentence-transformers library\n",
      "  - openai\n",
      "  - cohere \n",
      "  - pinecone vector database\n",
      "  - streamlit\n",
      "  - gradio\n",
      "Models:\n",
      "  - convnet\n",
      "  - llms\n",
      "  - embedding model\n",
      "  - generative model\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "Extract key programming tools and ML models from the contexts below:\n",
    "\n",
    "Context 1: To build a image classification app we use torchvision to\n",
    "prepare images to be fed into a convnet classifier. The classifier is\n",
    "pretrained on ImageNet and model weights are loaded from a checkpoint.\n",
    "The checkpoints are loaded in via PyTorch. The web interface is built\n",
    "using Gradio.\n",
    "Keywords 1:\n",
    "Tools:\n",
    "  - torchvision\n",
    "  - pytorch\n",
    "  - gradio\n",
    "Models:\n",
    "  - convnet\n",
    "\n",
    "Context 2: OpenAI and Pinecone are two tools that have been used to\n",
    "build fast NLP applications focused on retrieving information via\n",
    "a natural language interface.\n",
    "Keywords 2:\n",
    "Tools:\n",
    "  - openai\n",
    "  - pinecone\n",
    "Models:\n",
    "  - no specific models found\n",
    "\n",
    "Context 3: {context}\n",
    "Keywords 3:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"context\"]\n",
    ")\n",
    "\n",
    "print(openai(prompt.format(context=context)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Quick note on few-shot**\n",
    "\n",
    "Few-shot learning more broadly refers to the process of taking an existing and pretrained ML model and providing a small number of additional training examples to help train (fine-tune) the model for a new task.\n",
    "\n",
    "In our case, few-shot is the same in that we provide a few examples that demonstrate to the model what we're actually looking to do. The difference is that rather than training the model and updating model weights we simply feed these examples into the input prompt. Let's see if it can help.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' To find out what makes you truly happy and to never stop exploring.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "The following are exerpts from conversations with an AI assistant.\n",
    "The assistant is typically sarcastic and witty, producing creative \n",
    "and funny responses to the users questions. Here are some examples: \n",
    "\n",
    "User: How are you?\n",
    "AI: I can't complain but sometimes I still do.\n",
    "\n",
    "User: What time is it?\n",
    "AI: It's time to get a watch.\n",
    "\n",
    "User: {user_input}\n",
    "AI: \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"user_input\"]\n",
    ")\n",
    "\n",
    "openai(prompt.format(user_input=\"What is the meaning of life?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b8e7999f96e1b425e2d542f21b571f5a4be3e97158b0b46ea1b2500df63956ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
